{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea62656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().endswith('notebooks'): os.chdir('..')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from src.model import get_model\n",
    "from src.data import get_data, get_ood_data\n",
    "from src.ood_scores import OODEvaluator\n",
    "from src.neural_collapse import compute_nc_metrics\n",
    "\n",
    "from config import MODELS_DIR, BATCH_SIZE\n",
    "\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47450de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_data(BATCH_SIZE)\n",
    "ood_loader = get_ood_data(BATCH_SIZE)\n",
    "\n",
    "model_state = torch.load(os.path.join(MODELS_DIR, \"resnet18_cifar100.pth\"))\n",
    "model = get_model().to(device)\n",
    "model.load_state_dict(model_state[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = OODEvaluator(model)\n",
    "features, logits = evaluator.get_features_and_logits(test_loader, device)\n",
    "test_labels = torch.cat([l for _, l in test_loader])\n",
    "weights = model.base_model.fc.weight.detach().cpu()\n",
    "\n",
    "metrics = compute_nc_metrics(features.cpu(), test_labels.cpu(), weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da4387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nc1(features, labels, means):\n",
    "    \"\"\"\n",
    "    Visualizes NC1 Analysis: Intra-class variability collapse.\n",
    "\n",
    "    This function quantifies the 'tightness' of class clusters by measuring the \n",
    "    ratio of within-class covariance to between-class covariance. In a perfect \n",
    "    Neural Collapse regime, this ratio converges toward zero.\n",
    "\n",
    "    Formula (Trace Ratio):\n",
    "        NC1 = Tr(Sigma_W * pinv(Sigma_B)) / K\n",
    "        Where:\n",
    "        Sigma_W = 1/N * sum_{c=1}^K sum_{i=1}^{N_c} (f_{i,c} - mu_c)(f_{i,c} - mu_c)^T\n",
    "        Sigma_B = 1/K * sum_{c=1}^K (mu_c - mu_G)(mu_c - mu_G)^T\n",
    "\n",
    "    Args:\n",
    "        features (torch.Tensor): Latent representations (embeddings), shape [N, d].\n",
    "        labels (torch.Tensor): Ground truth class indices, shape [N].\n",
    "        means (torch.Tensor): Computed class centroids, shape [K, d].\n",
    "    \"\"\"\n",
    "\n",
    "    intra_distances = []\n",
    "    num_classes = means.shape[0]\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        mask = (labels == i)\n",
    "        if mask.sum() > 0:\n",
    "            diff = features[mask] - means[i]\n",
    "            intra_distances.append(torch.norm(diff, dim=1))\n",
    "    \n",
    "    all_intra = torch.cat(intra_distances)\n",
    "    \n",
    "    mu_g = means.mean(dim=0)\n",
    "    inter_dist = torch.norm(means - mu_g, dim=1).mean()\n",
    "    \n",
    "    # Ratio < 1 = cluster is tighter than the global spread\n",
    "    collapsed_distances = (all_intra / inter_dist).cpu().numpy()\n",
    "    avg_nc1 = collapsed_distances.mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 7), facecolor='white')\n",
    "    \n",
    "    n, bins, patches = plt.hist(\n",
    "        collapsed_distances, bins=80, \n",
    "        color='#4a69bd', alpha=0.7, \n",
    "        edgecolor='#2f3542', linewidth=0.5\n",
    "    )\n",
    "\n",
    "    plt.axvline(avg_nc1, color='#e74c3c', linestyle='--', linewidth=2, label=f'Mean NC1 Ratio: {avg_nc1:.4f}')\n",
    "\n",
    "    plt.title(\"NC1: Intra-class Variability Collapse\", fontsize=16, fontweight='normal', pad=20)\n",
    "    plt.xlabel(\"Relative Distance (Intra-class spread / Inter-class separation)\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Samples\", fontsize=12)\n",
    "    \n",
    "    plt.xlim(0, min(max(collapsed_distances), avg_nc1 * 3))\n",
    "    \n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().yaxis.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "    plt.text(avg_nc1 * 1.1, max(n) * 0.8, \n",
    "             f\"Ideal NC1 $\\\\rightarrow$ 0\\nMeasured: {avg_nc1:.4f}\", \n",
    "             fontsize=11, color='#c0392b', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    plt.legend(frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return avg_nc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nc2(means, grid_step=10):\n",
    "    \"\"\"\n",
    "    Visualizes NC2 Analysis: Convergence of class means to a Simplex ETF.\n",
    "\n",
    "    This function plots the cosine similarity matrix between centered class means.\n",
    "    In a Neural Collapse regime, the off-diagonal entries should converge to \n",
    "    the ETF constant, reflecting maximal mutual repulsion.\n",
    "\n",
    "    Formula (Inter-class Cosine Similarity):\n",
    "        S_{i,j} = <mu_i - mu_G, mu_j - mu_G> / (||mu_i - mu_G|| * ||mu_j - mu_G||)\n",
    "        Ideal Target (NC2) = -1 / (K - 1)\n",
    "\n",
    "    Args:\n",
    "        means (torch.Tensor): Matrix of class centroids, shape [K, d].\n",
    "        grid_step (int): Interval for drawing visual grid lines on the heatmap \n",
    "            (default: 10 for CIFAR-100).\n",
    "    \"\"\"\n",
    "    \n",
    "    mu_g = means.mean(dim=0)\n",
    "    norm_means = F.normalize(means - mu_g, p=2, dim=1)\n",
    "    \n",
    "    cosine_sim = torch.mm(norm_means, norm_means.T).cpu().numpy()\n",
    "    num_classes = means.shape[0]\n",
    "    etf_target = -1 / (num_classes - 1)\n",
    "    \n",
    "    plt.figure(figsize=(11, 9), facecolor='white')\n",
    "    \n",
    "    # Normalization focused on the ETF target\n",
    "    norm = TwoSlopeNorm(vcenter=etf_target, vmin=-1.0, vmax=1.0)\n",
    "    img = plt.imshow(cosine_sim, cmap='RdBu_r', norm=norm, interpolation='nearest')\n",
    "\n",
    "    mask = ~torch.eye(num_classes, dtype=bool)\n",
    "    off_diag_avg = cosine_sim[mask].mean().item()\n",
    "    print(f\"Experimental Mean Similarity: {off_diag_avg:.6f}\")\n",
    "    \n",
    "    for i in range(0, num_classes, grid_step):\n",
    "        plt.axhline(i - 0.5, color='black', linewidth=0.5, alpha=0.3)\n",
    "        plt.axvline(i - 0.5, color='black', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "    cbar = plt.colorbar(img)\n",
    "    cbar.set_label(\"Cosine Similarity\", fontsize=12)\n",
    "    \n",
    "    plt.title(f\"NC2: Simplex ETF Correlation Matrix\", \n",
    "              fontsize=16, fontweight='normal', pad=20)\n",
    "    plt.xlabel(\"Class Index\", fontsize=12)\n",
    "    plt.ylabel(\"Class Index\", fontsize=12)\n",
    "    \n",
    "    plt.xlim(-0.5, num_classes - 0.5)\n",
    "    plt.ylim(num_classes - 0.5, -0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nc3(model, means):\n",
    "    \"\"\"\n",
    "    Visualizes NC3 Analysis: Self-Duality (Weight-Centroid Alignment).\n",
    "\n",
    "    This function measures the alignment between the class centroids (means) \n",
    "    and the corresponding row vectors of the classifier's weight matrix. \n",
    "    In a collapsed state, the network exhibits self-duality, meaning the \n",
    "    classifier weights become equal to the centered class means (up to a scalar).\n",
    "\n",
    "    Formula (Alignment Metric):\n",
    "        NC3 = || W_c / ||W_c||_2 - (mu_c - mu_G) / ||mu_c - mu_G||_2 ||_2\n",
    "        As Neural Collapse progresses, NC3 -> 0 for all classes c=1...K.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained neural network containing \n",
    "            the final linear classification layer.\n",
    "        means (torch.Tensor): Computed class centroids in the feature \n",
    "            space, shape [K, d].\n",
    "    \"\"\"\n",
    "\n",
    "    weights = model.base_model.fc.weight.detach().cpu()\n",
    "    w_norm = F.normalize(weights, p=2, dim=1)\n",
    "    \n",
    "    mu_g = means.cpu().mean(dim=0)\n",
    "    m_centered = means.cpu() - mu_g\n",
    "    m_norm = F.normalize(m_centered, p=2, dim=1)\n",
    "    \n",
    "    cross_sim = torch.mm(w_norm, m_norm.T)\n",
    "    \n",
    "    best_alignment, _ = cross_sim.max(dim=1)\n",
    "    avg_nc3 = best_alignment.mean().item()\n",
    "\n",
    "    plt.figure(figsize=(10, 7), facecolor='white')\n",
    "    plt.hist(best_alignment.numpy(), bins=30, color='#27ae60', \n",
    "             alpha=0.6, edgecolor='#1e8449', linewidth=1.2)\n",
    "    \n",
    "    plt.axvline(avg_nc3, color='#c0392b', linestyle='--', linewidth=2, \n",
    "                label=f'Mean Alignment: {avg_nc3:.4f}')\n",
    "    \n",
    "    plt.title(\"NC3: Classifier Weight and Class Mean Alignment (Self-Duality)\", \n",
    "              fontsize=15, fontweight='normal', pad=20)\n",
    "    plt.xlabel(\"Cosine Similarity\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Classes\", fontsize=12)\n",
    "    \n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.gca().yaxis.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.legend(frameon=True, fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_nc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_features(model, layer, loader, device):\n",
    "    \"\"\"\n",
    "    Extracts latent features from a specific model layer and their associated labels.\n",
    "\n",
    "    This utility uses a forward hook to capture internal activations during \n",
    "    inference, which are essential for analyzing the geometric properties of \n",
    "    the latent space (Neural Collapse) and computing OOD scores.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network to extract features from.\n",
    "        layer (torch.nn.Module): The specific layer to hook (e.g., model.base_model.avgpool).\n",
    "        loader (torch.utils.data.DataLoader): The data source to process.\n",
    "        device (torch.device): The computing device (cuda or cpu).\n",
    "\n",
    "    Returns:\n",
    "        tuple (torch.Tensor, torch.Tensor): A pair of tensors (features, labels)\n",
    "            - features: The extracted activations, shape [N, d].\n",
    "            - labels: The corresponding ground truth class indices, shape [N].\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    labels_list = []\n",
    "    tmp_storage = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # output.shape : [B, C, H, W]\n",
    "        pooled_output = F.adaptive_avg_pool2d(output, (1, 1)).flatten(1) # [B, C]\n",
    "        tmp_storage.append(pooled_output.detach().cpu())\n",
    "\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            model(images)\n",
    "            labels_list.append(labels.cpu())\n",
    "            \n",
    "    handle.remove()\n",
    "\n",
    "    features = torch.cat(tmp_storage, dim=0)\n",
    "    all_labels = torch.cat(labels_list, dim=0)\n",
    "\n",
    "    return features, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nc4(model, test_loader, class_means, device):\n",
    "    \"\"\"\n",
    "    Visualizes NC4 Analysis: Equivalence to Nearest Class Center (NCC) Rule.\n",
    "\n",
    "    This function evaluates the convergence of the learned Softmax classifier \n",
    "    to a simplified Nearest Class Center (NCC) decision rule. In the NC regime, \n",
    "    the complex decision boundaries of the neural network simplify into Voronoi \n",
    "    cells centered around the class centroids.\n",
    "\n",
    "    Formula (Decision Equivalence):\n",
    "        y_softmax = argmax_c (W^T * f + b)\n",
    "        y_ncc = argmin_c || f - mu_c ||_2\n",
    "        Metric: | Accuracy(y_softmax) - Accuracy(y_ncc) | -> 0\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained neural network.\n",
    "        test_loader (torch.utils.data.DataLoader): Data to evaluate accuracy.\n",
    "        class_means (torch.Tensor): Computed class centroids, shape [K, d].\n",
    "        device (torch.device): Computing device (cuda or cpu).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            features = output[0] if isinstance(output, tuple) else output\n",
    "            logits = output[1] if isinstance(output, tuple) else None\n",
    "            \n",
    "            all_features.append(features.cpu())\n",
    "            if logits is not None:\n",
    "                all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    test_features = torch.cat(all_features)\n",
    "    test_labels = torch.cat(all_labels)\n",
    "    \n",
    "    # Softmax acc\n",
    "    if len(all_logits) > 0:\n",
    "        test_logits = torch.cat(all_logits)\n",
    "        acc_softmax = (test_logits.argmax(dim=1) == test_labels).float().mean().item() * 100\n",
    "    else: acc_softmax = 0.0 \n",
    "\n",
    "    # NCC acc\n",
    "    mu_g = class_means.mean(dim=0).cpu()\n",
    "    feat_norm = F.normalize(test_features - mu_g, p=2, dim=1)\n",
    "    means_norm = F.normalize(class_means.cpu() - mu_g, p=2, dim=1)\n",
    "    \n",
    "    cos_sim = torch.mm(feat_norm, means_norm.T)\n",
    "    preds_ncc = cos_sim.argmax(dim=1)\n",
    "    acc_ncc = (preds_ncc == test_labels).float().mean().item() * 100\n",
    "\n",
    "    plt.figure(figsize=(9, 7), facecolor='white')\n",
    "    labels = ['Softmax (Network)', 'NCC (Geometric)']\n",
    "    accuracies = [acc_softmax, acc_ncc]\n",
    "    \n",
    "    colors = ['#4a69bd', '#7f8c8d']\n",
    "    bars = plt.bar(labels, accuracies, color=colors, alpha=0.8, width=0.4)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1.5,\n",
    "                 f'{height:.2f}%', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.ylim(0, 110)\n",
    "    gap = abs(acc_softmax - acc_ncc)\n",
    "    plt.title(f\"NC4: Decision Rule Convergence (Gap: {gap:.4f}%)\", \n",
    "              fontsize=15, fontweight='normal', pad=20)\n",
    "    plt.ylabel(\"Test Accuracy (%)\", fontsize=12)\n",
    "    \n",
    "    plt.gca().yaxis.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return acc_softmax, acc_ncc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f81a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progressive_pca(model, loader, device):\n",
    "    \"\"\"\n",
    "    Visualizes the latent space organization using a 2x2 PCA grid.\n",
    "\n",
    "    This function provides a high-contrast qualitative assessment of the \n",
    "    feature space. It projects the high-dimensional embeddings into a \n",
    "    2D principal subspace to observe class clustering and the emergence \n",
    "    of the Simplex ETF geometry.\n",
    "\n",
    "    Formula (PCA Projection):\n",
    "        f_centered = f - E[f]\n",
    "        Components = Eigenvectors of (f_centered^T * f_centered)\n",
    "        f_2D = f_centered @ V_{1:2}\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model to extract features from.\n",
    "        loader (torch.utils.data.DataLoader): The dataset used for visualization.\n",
    "        device (torch.device): Computing device (cuda or cpu).\n",
    "    \"\"\"\n",
    "    \n",
    "    layers = [\n",
    "        model.base_model.layer1, \n",
    "        model.base_model.layer2, \n",
    "        model.base_model.layer3, \n",
    "        model.base_model.layer4\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12), facecolor='white')\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    target_classes = [0, 10, 20, 30, 40] \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "    for idx, layer in enumerate(layers):\n",
    "        feats, lbls = extract_layer_features(model, layer, loader, device)\n",
    "        \n",
    "        mask = torch.tensor([l in target_classes for l in lbls])\n",
    "        f_sub = feats[mask].numpy()\n",
    "        l_sub = lbls[mask].numpy()\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        projected = pca.fit_transform(f_sub)\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        for i, c in enumerate(target_classes):\n",
    "            c_mask = l_sub == c\n",
    "            ax.scatter(\n",
    "                projected[c_mask, 0], \n",
    "                projected[c_mask, 1], \n",
    "                color=colors[i],\n",
    "                label=f'Class {c}', \n",
    "                s=25, \n",
    "                alpha=0.7, \n",
    "                edgecolors='w', \n",
    "                linewidths=0.3\n",
    "            )\n",
    "        \n",
    "        ax.set_box_aspect(1) \n",
    "        \n",
    "        ax.set_title(f\"Layer {idx+1}\", fontsize=16, fontweight='normal', pad=15)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_color('#888888')\n",
    "            spine.set_linewidth(1.0)\n",
    "\n",
    "    plt.suptitle(\"Geometric Evolution and Neural Collapse Across Layers\", \n",
    "                 fontsize=22, fontweight='normal', y=0.96)\n",
    "    \n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.05),\n",
    "               ncol=5, fontsize=13, frameon=True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.93])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_final, labels_final = extract_layer_features(model, model.base_model.layer4, test_loader, device)\n",
    "\n",
    "class_means = []\n",
    "for i in range(100):\n",
    "    mask = (labels_final == i)\n",
    "    class_means.append(features_final[mask].mean(dim=0))\n",
    "class_means = torch.stack(class_means)\n",
    "\n",
    "fc_weights = model.base_model.fc.weight.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a5f24",
   "metadata": {},
   "source": [
    "## NC1: Intra-class Variability Collapse\n",
    "\n",
    "This plot illustrates the distance between individual sample features and their respective class centroids. \n",
    "\n",
    "* **Interpretation**: A distribution that is both narrow and centered near zero indicates that the network has successfully \"collapsed\" intra-class variations. \n",
    "* **Theoretic Goal**: This process effectively erases non-discriminative information, mapping all samples of the same category onto a single point (the centroid) in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nc1(features_final, labels_final, class_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b424f",
   "metadata": {},
   "source": [
    "## NC2: Convergence to Simplex ETF\n",
    "\n",
    "This heatmap displays the cosine similarity matrix between centered class means. \n",
    "\n",
    "* **Observation**: We look for a uniform off-diagonal structure.\n",
    "* **Theoretical Target**: In a state of Neural Collapse, class centers reach a state of maximal mutual repulsion. We expect the off-diagonal entries to converge toward the theoretical constant of $-1/(K-1)$ (approx. $-0.01$ for CIFAR-100), forming a perfectly symmetrical **Simplex Equiangular Tight Frame (ETF)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nc2(class_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ba946",
   "metadata": {},
   "source": [
    "## NC3 & NC4: Structural Alignment and Decision Rule\n",
    "\n",
    "These visualizations compare the learned classifier weights with the feature centroids and the equivalence of the decision boundaries.\n",
    "\n",
    "* **NC3 (Self-Duality)**: As the latent space reaches its optimal geometric configuration, the classifier weights align perfectly with the class means.\n",
    "* **NC4 (NCC Equivalence)**: The complex Softmax decision boundaries simplify into **Voronoi cells**. At this stage, a simple **Nearest Class Center (NCC)** rule performs as well as the fully trained linear head, confirming the geometric hollowing of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ce13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nc3(model, class_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nc4(model, test_loader, class_means, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee7240",
   "metadata": {},
   "source": [
    "## Visual Interpretation (PCA)\n",
    "\n",
    "The 2D projection of the high-dimensional feature space provides a qualitative confirmation of the NC phenomenon.\n",
    "\n",
    "> **Key takeaway**: We observe the transition from a disorganized cloud of points to highly concentrated, equidistant clusters. This structural prior is what we leverage to improve Out-of-Distribution detection: by forcing In-Distribution data into these rigid \"anchors,\" OOD samples are more likely to fall into the \"geometric void\" between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progressive_pca(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91318b8",
   "metadata": {},
   "source": [
    "## NC5: Feature Invariance under Augmentation\n",
    "\n",
    "NC5 measures the stability of the latent representations when the input undergoes stochastic transformations (rotations, crops, jittering).\n",
    "\n",
    "* **Observation**: In the \"Star Plot\", each cluster represents a single source image and its variations. The smaller the \"star\" radius, the higher the invariance.\n",
    "* **Interpretation**: A successful Neural Collapse implies that the network has learned to ignore the \"manifold of variations\" (noise, pose, lighting). It collapses all possible versions of an image into a single, robust semantic point.\n",
    "* **Link to OOD**: High NC5 invariance ensures that the In-Distribution (ID) anchors are extremely stable. This makes any Out-of-Distribution (OOD) sample—which won't map to these stable points—much easier to distinguish as it falls outside these tightly collapsed zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a82e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_nc5_invariance(model, image, transform_pipeline, n_augmentations=50):\n",
    "    \"\"\"\n",
    "    Analyzes NC5 Invariance: Feature representation stability under augmentation.\n",
    "\n",
    "    This function quantifies the degree to which the model's latent features are \n",
    "    invariant to data augmentations. In a Neural Collapse regime, the network \n",
    "    learns to map all augmented versions of an image to its class centroid, \n",
    "    effectively collapsing the augmentation manifold.\n",
    "\n",
    "    Formula (Invariance Metric):\n",
    "        Let f_0 be the embedding of the original image and f_j the j-th augmentation.\n",
    "        NC5_score = (1 / n) * sum_{j=1}^n || f_j - f_0 ||_2\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained neural network.\n",
    "        image (torch.Tensor): A single seed image, shape [C, H, W].\n",
    "        transform_pipeline (callable): A stochastic augmentation pipeline.\n",
    "        n_augmentations (int): Number of augmented samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean Euclidean distance between the augmented embeddings \n",
    "               and the original image's centroid (NC5 score).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    augmented_images = torch.stack([transform_pipeline(image) for _ in range(n_augmentations)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(augmented_images)\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            features = output[0] if output[0].dim() == 2 else output[1]\n",
    "        else: features = output\n",
    "            \n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "        \n",
    "    sim_matrix = torch.matmul(features, features.T).cpu().numpy()\n",
    "    \n",
    "    centroid = features.mean(dim=0, keepdim=True)\n",
    "    centroid = F.normalize(centroid, p=2, dim=1)\n",
    "    dist_to_centroid = torch.norm(features - centroid, dim=1).cpu().numpy()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    im = ax1.imshow(sim_matrix, cmap='viridis', vmin=0.8, vmax=1.0)\n",
    "    ax1.set_title(\"Similarité Cosinus entre Augmentations\\n(Proche de 1 = NC5 fort)\")\n",
    "    plt.colorbar(im, ax=ax1)\n",
    "    \n",
    "    ax2.hist(dist_to_centroid, bins=20, color='salmon', edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(dist_to_centroid.mean(), color='red', linestyle='--', \n",
    "        label=f'Dispersion moyenne: {dist_to_centroid.mean():.4f}')\n",
    "    ax2.set_title(\"Dispersion Intra-échantillon (NC5)\")\n",
    "    ax2.set_xlabel(\"Distance Euclidienne au centroïde de l'image\")\n",
    "    ax2.set_ylabel(\"Nombre d'augmentations\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return dist_to_centroid.mean()\n",
    "\n",
    "test_invariance_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(32, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "sample_image = next(iter(train_loader))[0].cpu()[0]\n",
    "\n",
    "avg_dispersion = analyze_nc5_invariance(\n",
    "    model=model, \n",
    "    image=sample_image, \n",
    "    transform_pipeline=test_invariance_transform, \n",
    "    n_augmentations=100\n",
    ")\n",
    "\n",
    "print(f\"Dispersion moyenne NC5 : {avg_dispersion:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90dd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nc5_stars(model, dataloader, transform_pipeline, n_images=6, n_augmentations=40):\n",
    "    \"\"\"\n",
    "    Visualizes NC5 (Feature Invariance) using a 2D projection of augmented samples.\n",
    "\n",
    "    This function generates a \"Star Plot\" where each star corresponds to one \n",
    "    original image. The center of the star is the original embedding, and the \n",
    "    points are its stochastic augmentations. Shorter \"rays\" indicate higher \n",
    "    invariance to data transformations.\n",
    "\n",
    "    Formula (Geometric Interpretation):\n",
    "        Original: f_0 = Encoder(x)\n",
    "        Augmented: f_j = Encoder(T_j(x))\n",
    "        Star Radius: d_j = || f_j - f_0 ||_2\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained neural network.\n",
    "        dataloader (torch.utils.data.DataLoader): Source for the seed images.\n",
    "        transform_pipeline (callable): Stochastic augmentation pipeline (e.g., RandAugment).\n",
    "        n_images (int): Number of distinct stars (source images) to plot.\n",
    "        n_augmentations (int): Number of augmented samples per source image.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    images, labels = next(iter(dataloader))\n",
    "    all_features = []\n",
    "    group_ids = []\n",
    "    class_names = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_images):\n",
    "            aug_imgs = torch.stack([transform_pipeline(images[i]) for _ in range(n_augmentations)]).to(device)\n",
    "            \n",
    "            output = model(aug_imgs)\n",
    "            f = output[0] if isinstance(output, tuple) and output[0].dim() == 2 else output\n",
    "            if isinstance(f, tuple): f = f[1]\n",
    "            \n",
    "            f = F.normalize(f, p=2, dim=1)\n",
    "            \n",
    "            all_features.append(f.cpu())\n",
    "            group_ids.extend([i] * n_augmentations)\n",
    "            class_names.append(labels[i].item())\n",
    "\n",
    "    features_concat = torch.cat(all_features).numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    features_2d = pca.fit_transform(features_concat)\n",
    "\n",
    "    plt.figure(figsize=(12, 8), facecolor='white')\n",
    "    colors = plt.cm.get_cmap('Dark2', n_images)\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        idx = np.where(np.array(group_ids) == i)[0]\n",
    "        points = features_2d[idx]\n",
    "        center = points.mean(axis=0)\n",
    "        \n",
    "        for p in points:\n",
    "            plt.plot([center[0], p[0]], [center[1], p[1]], \n",
    "                     color=colors(i), alpha=0.15, lw=1, zorder=1)\n",
    "        \n",
    "        plt.scatter(points[:, 0], points[:, 1], \n",
    "                    color=colors(i), s=25, alpha=0.6, edgecolors='none', zorder=2)\n",
    "        \n",
    "        plt.scatter(center[0], center[1], \n",
    "                    color=colors(i), s=150, marker='o', edgecolors='black', \n",
    "                    linewidth=1.5, label=f\"Image {i} (Class {class_names[i]})\", zorder=3)\n",
    "\n",
    "    plt.title(\"NC5 Visualization: Feature Invariance to Augmentations\", fontsize=16, pad=20)\n",
    "    plt.xlabel(\"PCA Principal Component 1\", fontsize=12)\n",
    "    plt.ylabel(\"PCA Principal Component 2\", fontsize=12)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Samples\", frameon=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_nc5_stars(\n",
    "    model=model, \n",
    "    dataloader=train_loader, \n",
    "    transform_pipeline=test_invariance_transform,\n",
    "    n_images=5, \n",
    "    n_augmentations=50\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
